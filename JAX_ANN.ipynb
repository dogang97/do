{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogang97/do/blob/main/JAX_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHJSGlcJ-ubC"
      },
      "source": [
        "산업수학 기반 데이터 분석 프로그램\n",
        "\n",
        "김병천 (wizardbc@gmail.com)<br>\n",
        "인하대학교 응용수학연구소 / 스마트소셜 AI 연구소\n",
        "\n",
        "2022년 8월 17일 (수)<br>\n",
        "더케이호텔 서울, 금강A홀\n",
        "\n",
        "# Artificial Neural Network (ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgEsABIe-yXb"
      },
      "source": [
        "![ANN](https://upload.wikimedia.org/wikipedia/commons/3/3d/Neural_network.svg)\n",
        "\n",
        "[Wikimedia](https://commons.wikimedia.org/wiki/File:Neural_network.svg)\n",
        "\n",
        "* Each circle represents a real number.\n",
        "* The input neurons (the green circles) represent a vector $(x_1, x_2)^T\\in\\mathbb R^2$.\n",
        "* The edges from the input neurons to the hidden neurons (the blue circles) represent weights $\\left(a_{ij}^{(1)}\\right)$ and biases $\\left(b_{ij}^{(1)}\\right)$ in $\\mathbb{R}^{5\\times 2}$.\n",
        "* The hidden neurons (the blue circles) represent a vector $\\left(\\sigma(y_1), \\ldots, \\sigma(y_5)\\right)^T\\in\\mathbb R^5$ where $y_i = \\sum_{j=1}^2\\left(a_{ij}^{(1)}x_j+b_{ij}^{(1)}\\right)$ and $\\sigma:\\mathbb R\\rightarrow\\mathbb R$ is an activation function.\n",
        "* The edges from the hidden neurons to output neurons (the yello circle) represent weights $\\left(a_{ij}^{(2)}\\right)$ and biases $\\left(b_{ij}^{(2)}\\right)$ in $\\mathbb{R}^{1\\times 5}$.\n",
        "* The output neurons represent a real number $\\sum_{j=1}^5 \\left(a_{ij}^{(2)}\\sigma(y_i)+b_{ij}^{(2)}\\right)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pphj3why6TI"
      },
      "source": [
        "## Affine Maps\n",
        "\n",
        "If $x=(x_1,\\ldots, x_d)^T\\in\\mathbb R^d$, an affine map $W$ consists of linear map (matrix) $A=\\left(a_{ij}\\right)\\in\\mathbb R^{n\\times d}$ (weights) and vector $b=(b_1,\\ldots,b_n)^T\\in\\mathbb R^n$ (biases). Then $W(x)$ means $n$-dimensional vector $Ax+b=\\left(\\sum_{j=1}^d a_{ij}x_j+b_i\\right)^T\\in\\mathbb R^n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UpGSUT9y9Zi"
      },
      "source": [
        "## Component-wise Composition\n",
        "\n",
        "Given a function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$, and $y=\\left(y_1,\\ldots,y_n\\right)^T\\in\\mathbb R^n$, $\\sigma(y)$ means $\\left(\\sigma(y_1),\\ldots,\\sigma(y_n)\\right)^T\\in\\mathbb R^n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4cvy9VrnQCq"
      },
      "source": [
        "Now we can write the neural network in the picture by\n",
        "$$W_2\\cdot\\sigma\\circ W_1$$\n",
        "where\n",
        "*  $W_1 = \\left(A_1, b_1\\right)$ with $A_1=\\left(a_{ij}^{(1)}\\right)\\in\\mathbb R^{5\\times 2}$ and $b_1=\\left(\\sum_{j=1}^2 b_{ij}^{(1)}\\right)^T\\in\\mathbb R^5$,\n",
        "*  $W_2 = \\left(A_2, b_2\\right)$ with $A_2=\\left(a_{ij}^{(2)}\\right)\\in\\mathbb R^{1\\times 5}$ and $b_2=\\sum_{j=1}^5 b_{1j}^{(2)}\\in\\mathbb R^1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN9JRg6jUoBa"
      },
      "source": [
        "# Universal Approximation Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Co8g9fXVNn"
      },
      "source": [
        "## Arbitrary-width case\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-width_case)\n",
        "\n",
        "(1989, George Cybenko)\n",
        "\n",
        "Fix a continuous function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$ (activation function) and positive integers $d, D$. The function $\\sigma$ is not a polynomial if and only if, for every continuous function $f:\\mathbb R^d\\rightarrow\\mathbb R^D$ (target function), every compact subset $K$ of $\\mathbb R^d$, and every $\\epsilon > 0$ there exists a continuous function $f_\\epsilon:\\mathbb R^d\\rightarrow\\mathbb R^D$ with representation $$f_\\epsilon = W_2\\cdot\\sigma\\circ W_1,$$\n",
        "where $W_1, W_2$ are composable affine maps and $\\circ$ denotes component-wise composition, such that the approximation bound $$\\sup_{x\\in K}\\|f(x)-f_\\epsilon(x)\\|<\\epsilon.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVHN74K5YixU"
      },
      "source": [
        "### Sketch\n",
        "Given continuous $\\sigma:\\mathbb R\\rightarrow\\mathbb R$, positive integers $d, D$.\n",
        "\n",
        "$\\sigma$ is not a polynomial $\\iff$\n",
        "* $\\forall f:\\mathbb R^d\\rightarrow \\mathbb R^D$,\n",
        "* $\\forall$ compact $K\\subset \\mathbb R^d$,\n",
        "* $\\forall \\epsilon>0$,\n",
        "\n",
        "there exists a neural network $\\hat f$:\n",
        "* $d$ input neurons,\n",
        "* $D$ output neurons,\n",
        "* only one hidden layer with an arbitrary number of hidden neurons having activation function $\\sigma$,\n",
        "\n",
        "such that\n",
        "$$\\sup_{x\\in K}\\|f(x)-\\hat f(x)\\|<\\epsilon.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ePLq5dzBfa"
      },
      "source": [
        "#### Unit Step Function (Heaviside Step Function)\n",
        "\n",
        "$$H(x) := \\begin{cases} 1, & x > 0 \\\\ 0, & x \\le 0 \\end{cases}$$\n",
        "Sometimes,\n",
        "$$H(x) := \\begin{cases} 1, & x > 0 \\\\ \\frac{1}{2}, & x = 0 \\\\ 0, & x < 0 \\end{cases}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubQUCin7P6Cr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXy4X9ziRmA4"
      },
      "source": [
        "plt.style.use('dark_background')\n",
        "\n",
        "# H = np.vectorize(lambda x: 1 if x > 0.0 else 0.0)\n",
        "H = np.vectorize(lambda x: 1/2 if x==0.0 else 1 if x>0.0 else 0.0)\n",
        "\n",
        "domain = np.linspace(-10,10,1001)\n",
        "plt.scatter(domain, H(domain), s=.1)\n",
        "plt.title(\"Unit step function\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx43Q2HU70oR"
      },
      "source": [
        "#### Boxcar Function\n",
        "\n",
        "$$\\Pi_{a,b}(x):=H(x-a)-H(x-b)$$\n",
        "We can approximate any continuous function $f:\\mathbb R\\rightarrow\\mathbb R$ in some closed interval $K=[s,e]$ using boxcar functions:\n",
        "\n",
        "Given any $\\epsilon > 0$, we can find positive integer $n$ such that if $$x_i := s+\\frac{e-s}{n}i \\qquad \\left(0\\leq i \\leq n\\right)$$\n",
        "and\n",
        "$$f_n:=\\sum_{i=1}^{n}f(x_{i})\\Pi_{x_{i-1},x_{i}}$$\n",
        "then\n",
        "$$\\sup_{x\\in K}\\|f(x)-f_n(x)\\|<\\epsilon.$$\n",
        "\n",
        "Moreover, since\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "f_n &= \\sum_{i=1}^{n}f(x_i)\\left(H(x-x_{i-1}) - H(x-x_i)\\right)\\\\\n",
        "&=f(x_1)H(x-x_{0})-\\sum_{i=1}^{n-1}\\left(f(x_i)-f(x_{i+1})\\right)H(x-x_i)-f(x_n)H(x-x_n),\n",
        "\\end{align*}\n",
        "$$\n",
        "we have\n",
        "$$f_n = W_2\\cdot H\\circ W_1$$\n",
        "where\n",
        "$$\n",
        "\\begin{align*}\n",
        "W_1 &= \\left((1,1,\\ldots,1)^T, \\left(-x_0,-x_1,\\ldots,-x_n\\right)^T\\right)\\in\\mathbb R^{(n+1)\\times 1}\\times\\mathbb R^{n+1},\\\\\n",
        "W_2 &= \\left(\\left(f(x_1), f(x_2)-f(x_1), f(x_3)-f(x_2),\\ldots,f(x_n)-f(x_{n-1}), -f(x_n)\\right), 0\\right)\\in\\mathbb R^{1\\times (n+1)}\\times\\mathbb R.\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daV1Hv-CTawh"
      },
      "source": [
        "boxcar = lambda x, a=-.5, b=.5: H(x-a) - H(x-b)\n",
        "\n",
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, boxcar(domain), s=.1)\n",
        "plt.title(\"Unit rectangle function\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHKGZIsk7XI4"
      },
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.scatter(domain, f(domain), s=.1, label='$f$')\n",
        "  plt.scatter(domain, fn, s=.1, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.scatter(domain, err, s=.1)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw5YNc9HNj7x"
      },
      "source": [
        "* We have approximated $f$ using a number of unit step functions.\n",
        "\n",
        "By replacing the closed interval $K$ to the product of closed intervals, we can show the general $f:\\mathbb R^d\\rightarrow\\mathbb R^D$ case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPBvUSXWr_49"
      },
      "source": [
        "#### Continuous Version of Unit Step Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkJVghGURPrr"
      },
      "source": [
        "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "\n",
        "domain = np.linspace(-10,10,1001)\n",
        "\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "plt.plot(domain, sigmoid(domain), label='sigmoid')\n",
        "plt.title(\"Sigmoid\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ir7gw9DRkDO"
      },
      "source": [
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "for a in [1, 2, 4, 8, 16, 32]:\n",
        "  plt.plot(domain, sigmoid(a*domain), label=f'a={a}')\n",
        "plt.title(\"Sigmoid(ax)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkU2MPG5Q3d1"
      },
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "sig_boxcar = lambda x, a, b: sigmoid(512*(x-a)) - sigmoid(512*(x-b))\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*sig_boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(domain, f(domain), label='$f$')\n",
        "  plt.plot(domain, fn, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.plot(domain, err)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB1UoEd2syqv"
      },
      "source": [
        "* We have approximated $f$ using a number of sigmoid functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T01fqX9xH3vg"
      },
      "source": [
        "relu = lambda x: x*(x>0)\n",
        "\n",
        "domain = np.linspace(-10,10,1000)\n",
        "\n",
        "plt.plot(domain, relu(domain))\n",
        "plt.title(\"ReLU\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM6zuCCkyECH"
      },
      "source": [
        "domain = np.linspace(-2,2,1001)\n",
        "plt.scatter(domain, H(domain), s=.1, label='unit step function')\n",
        "plt.plot([0],[.5]) # waste one color\n",
        "for a in [1, 2, 4, 8, 16, 32]:\n",
        "  plt.plot(domain, relu(a*domain+0.5) - relu(a*domain-0.5), label=f'a={a}')\n",
        "plt.title(\"ReLU(ax+0.5)-ReLU(ax-0.5)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv_EA6UTyoaD"
      },
      "source": [
        "f = lambda x: -1 * (x+1) * x * (x-1)\n",
        "\n",
        "relu_boxcar = lambda x, a, b: (relu(256*(x-a)+0.5) - relu(256*(x-a)-0.5)) - (relu(256*(x-b)+0.5) - relu(256*(x-b)-0.5))\n",
        "\n",
        "domain = np.linspace(0,1,10001)\n",
        "\n",
        "\n",
        "for n in [3, 5, 9, 17, 33, 65, 129, 257, 513]:\n",
        "  d = np.linspace(0,1,n)\n",
        "  a = d[:-1]\n",
        "  b = d[1:]\n",
        "  fn = sum([f(b)*relu_boxcar(domain,a,b) for a,b in list(zip(a,b))])\n",
        "\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(domain, f(domain), label='$f$')\n",
        "  plt.plot(domain, fn, label='approx.')\n",
        "  plt.title(f\"An approximation of $f$ (n={n})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  err = f(domain)-fn\n",
        "  plt.plot(domain, err)\n",
        "  plt.title(f\"Errors ({np.abs(err).max():.5f})\")\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvYLLg7es17Y"
      },
      "source": [
        "* We have approximated $f$ using a number of ReLU(Rectified Linear Unit) functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VatA_HB2bWG3"
      },
      "source": [
        "#### Summary\n",
        "* continuous function $f:\\mathbb R^d\\rightarrow\\mathbb R^D$\n",
        "* continuous non-polynomial function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$.\n",
        "\n",
        "Then $f$ can be approximated by a neural network represented by $W_2\\circ\\sigma\\circ W_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIt1JWAGjXCE"
      },
      "source": [
        "## Arbitrary-depth case\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-depth_case)\n",
        "\n",
        "(2017, Zhou Lu et al.)\n",
        "\n",
        "Let $\\mathcal{X}$ be a compact subset of $\\mathbb{R}^d$. Let $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ be any non-affine continuous function which is continuously differentiable at at-least one point, with non-zero derivative at that point. Let $\\mathcal{N}_{d,D:d+D+2}^{\\sigma}$ denote the space of feed-forward neural networks with $d$ input neurons, $D$ output neurons, and an arbitrary number of hidden layers each with $d + D + 2$ neurons, such that every hidden neuron has activation function $\\sigma$ and every output neuron has the identity as its activation function, with input layer $\\phi$, and output layer $\\rho$. Then given any $\\varepsilon>0$ and any $f\\in C(\\mathcal{X},\\mathbb{R}^D)$, there exists $\\hat{f}\\in \\mathcal{N}_{d,D:d+D+2}^{\\sigma}$ such that\n",
        "$$\n",
        "\\sup_{x \\in \\mathcal{X}}\\,\\left\\|\\hat{f}(x)-f(x)\\right\\| < \\varepsilon.\n",
        "$$\n",
        "\n",
        "In other words, $\\mathcal{N}$ is dense in $C(\\mathcal{X}; \\mathbb{R}^D)$ with respect to the uniform topology.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfdZadkkzDo"
      },
      "source": [
        "### Sketch\n",
        "Given a continuous function $\\sigma:\\mathbb R\\rightarrow\\mathbb R$\n",
        "\n",
        "$\\sigma$ is continuously differentiable at at-least one point, with non-zero derivative at that point\n",
        "\n",
        "$\\implies$\n",
        "* $\\forall f:\\mathbb R^d\\rightarrow \\mathbb R^D$,\n",
        "* $\\forall$ compact $\\mathcal X\\subset \\mathbb R^d$,\n",
        "* $\\forall \\epsilon>0$,\n",
        "\n",
        "there exists a neural network $\\hat f$:\n",
        "* $d$ input neurons,\n",
        "* $D$ output neurons,\n",
        "* an arbitrary number of hidden layers each with $d+D+2$ neurons having activation function $\\sigma$,\n",
        "\n",
        "satisfying\n",
        "$$\\sup_{x\\in\\mathcal X}\\|f(x)-\\hat f(x)\\|<\\epsilon.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NehxEhp4YfIP"
      },
      "source": [
        "# Some Lessons from Universal Approximation Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBaM4J9EyWlw"
      },
      "source": [
        "## JAX\n",
        "\n",
        "* 메뉴 -> 런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU\n",
        "* 아래의 셀을 실행시켰을때 다음과 같은 결과가 나와야 합니다.\n",
        "```\n",
        "Devices: [GpuDevice(id=0, process_index=0)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax import jax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Devices:\", jax.devices())"
      ],
      "metadata": {
        "id": "ugzWAQ3BXzTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Valued Function\n",
        "\n",
        "* $[0,1]$ part of cubic polynomial $$f(x)=-x(x-1)(x+2)$$\n",
        "\n",
        "* Our data is $$\\left\\{\\left(x_1, f(x_1)+\\epsilon_1\\right),\\ldots, \\left(x_{100}, f(x_{100})+\\epsilon_{100}\\right)\\right\\}$$\n",
        "where $\\epsilon_i$ are noise."
      ],
      "metadata": {
        "id": "4i1JPosiaFe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('dark_background')\n",
        "\n",
        "f = lambda x: -x*(x-1)*(x+1)\n",
        "x = jnp.linspace(0,1,1000).reshape(1000,1)\n",
        "\n",
        "rng_key = random.PRNGKey(2022)\n",
        "rng_key, subkey1, subkey2 = random.split(rng_key, 3)\n",
        "\n",
        "xs = random.uniform(subkey1, shape=(100,1))\n",
        "# noise = random.normal(subkey2, shape=(100,1)) * 0.01\n",
        "ys = f(xs) #+ noise\n",
        "\n",
        "plt.title('Train Dataset')\n",
        "plt.scatter(xs, ys, label='data')\n",
        "plt.plot(x, f(x), label='$y=f(x)$', c='red')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MJZQmlZMZIlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "* Input dim. = 1\n",
        "* Hidden dim. = 50\n",
        "* Output dim. = 1"
      ],
      "metadata": {
        "id": "WNqicIBAdDaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key, w1_key, w2_key = random.split(rng_key, 3)\n",
        "\n",
        "params = {\n",
        "    'W1': random.normal(w1_key, (1,50)),\n",
        "    'b1': jnp.zeros((1,50)),\n",
        "    'W2': random.normal(w2_key, (50,1)),\n",
        "    'b2': jnp.zeros((1,1))\n",
        "}\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def simple_ann(params, x):\n",
        "  w1, b1 = params.get('W1'), params.get('b1')\n",
        "  w2, b2 = params.get('W2'), params.get('b2')\n",
        "\n",
        "  x = jax.nn.sigmoid(x@w1 + b1)\n",
        "  return x@w2 + b2"
      ],
      "metadata": {
        "id": "3Hm2P9OnbI1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Before Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs, ys)\n",
        "\n",
        "plt.plot(x, simple_ann(params, x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs, simple_ann(params, xs))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2Fm_4GDfnAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is $\\hat y(x;\\theta)= \\sigma(x\\cdot W_1+b_1)\\cdot W_2+b_2$.\n",
        "\n",
        "The loss function is $J(x,y;\\theta) = E_\\theta (\\hat y-y)^2$.\n",
        "\n",
        "### Gradient Descent with Momentum\n",
        "\n",
        "$$\\theta_{\\textrm{new}} = \\theta - \\eta(\\nabla_\\theta J)(x,y;\\theta)+\\alpha\\Delta\\theta$$\n",
        "\n",
        "* $\\eta$ : learning rate (or step size).\n",
        "* $\\alpha$ : momentum."
      ],
      "metadata": {
        "id": "KCTaWbbvh5L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(params, x, y):\n",
        "  y_hat = simple_ann(params, x)\n",
        "  return jnp.mean((y_hat - y)**2)\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state, x, y):\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "  lr, momentum = opt_state['lr'], opt_state['momentum']\n",
        "  change_old = opt_state['change']\n",
        "  opt_state['change'] = change = jax.tree_map(lambda g,c: lr*g + momentum*c, grads, change_old)\n",
        "  params = jax.tree_map(lambda p, c: p - c, params, change)\n",
        "  return params, opt_state, loss"
      ],
      "metadata": {
        "id": "iJclcZBBh3IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = {\n",
        "    'lr': 0.1,\n",
        "    'momentum': 0.8,\n",
        "    'change': jax.tree_map(lambda g: .0, jax.grad(loss_fn)(params, xs, ys))\n",
        "    }\n",
        "\n",
        "print(f'Epoch : 0\\tloss:{loss_fn(params, xs, ys):.8f}')\n",
        "for i in range(200000):\n",
        "  params, opt_state, loss = update(params, opt_state, xs, ys)\n",
        "  if i%10000==9999:\n",
        "    print(f'Epoch : {i+1}\\tloss:{loss:.8f}')"
      ],
      "metadata": {
        "id": "mrKHwquRh3E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('After Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs, ys)\n",
        "\n",
        "plt.plot(x, simple_ann(params, x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs, simple_ann(params, xs))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "33ImPEkDlF9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ03EA0nhfDS"
      },
      "source": [
        "* Good Job! (Really?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM68JLRfiKRr"
      },
      "source": [
        "## Lesson 1\n",
        "\n",
        "ANN model CANNOT tell about outside of the comapct set - in this case, outside of $[0,1]$.\n",
        "\n",
        "Note that the universal approximation theorem says:<br>\n",
        "the approximation bound\n",
        "$$\\sup_{x\\in K}\\|f(x)-f_\\epsilon(x)\\|<\\epsilon$$\n",
        "where $K\\subset\\mathbb{R}^D$ is a compact subset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNs6IpIwaHdu"
      },
      "source": [
        "plt.title('Outside of the campact set')\n",
        "xx = jnp.linspace(-1,1.5,2500)[:,jnp.newaxis]\n",
        "plt.plot(xx, f(xx),label='$y=f(x)$')\n",
        "plt.scatter(xs, ys)\n",
        "\n",
        "plt.plot(xx, simple_ann(params, xx), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs, simple_ann(params, xs))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D6sXBiSmJk4"
      },
      "source": [
        "## Lesson 2\n",
        "\n",
        "You should have enough data.\n",
        "\n",
        "If we have only 4 points of the cubic polynomial, then we get a bad approximation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SNmz6TimeGB"
      },
      "source": [
        "rng_key = random.PRNGKey(2)\n",
        "rng_key, subkey = random.split(rng_key)\n",
        "xs2 = random.uniform(subkey, shape=(4,1))\n",
        "ys2 = f(xs2)\n",
        "\n",
        "plt.title('Train Dataset')\n",
        "plt.plot(x, f(x), label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key, w1_key, w2_key = random.split(rng_key, 3)\n",
        "\n",
        "params2 = {\n",
        "    'W1': random.normal(w1_key, (1,50)),\n",
        "    'b1': jnp.zeros((1,50)),\n",
        "    'W2': random.normal(w2_key, (50,1)),\n",
        "    'b2': jnp.zeros((1,1))\n",
        "}"
      ],
      "metadata": {
        "id": "lhvRToBq4c3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Before Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2)\n",
        "\n",
        "plt.plot(x, simple_ann(params2, x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs2, simple_ann(params2, xs2))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "REbrOcaz4kto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state2 = {\n",
        "    'lr': 0.1,\n",
        "    'momentum': 0.8,\n",
        "    'change': jax.tree_map(lambda g: .0, jax.grad(loss_fn)(params2, xs2, ys2))\n",
        "    }\n",
        "\n",
        "print(f'Epoch : 0\\tloss:{loss_fn(params2, xs2, ys2):.8f}')\n",
        "for i in range(200000):\n",
        "  params2, opt_state2, loss = update(params2, opt_state2, xs2, ys2)\n",
        "  if i%10000==9999:\n",
        "    print(f'Epoch : {i+1}\\tloss:{loss:.8f}')"
      ],
      "metadata": {
        "id": "nY3Odpxd4wOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('After Train')\n",
        "\n",
        "plt.plot(x, f(x),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2)\n",
        "\n",
        "plt.plot(x, simple_ann(params2, x), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs2, simple_ann(params2, xs2))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jjjB9e5M-t1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('After Train')\n",
        "\n",
        "plt.plot(xx, f(xx),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2)\n",
        "\n",
        "plt.plot(xx, simple_ann(params2, xx), label='$y=\\hat{f}(x)$')\n",
        "plt.scatter(xs2, simple_ann(params2, xs2))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oSm-obWy_x96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1YJuC5nRcY"
      },
      "source": [
        "## Lesson 3\n",
        "\n",
        "***First of all***, you have to check if there exists a exact solution for the problem.\n",
        "\n",
        "Because we are working on cubic polynomial, 4 points are enough.\n",
        "\n",
        "Just solve the equation:\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "x_1^3 & x_1^2 & x_1 & 1\\\\\n",
        "x_2^3 & x_2^2 & x_2 & 1\\\\\n",
        "x_3^3 & x_3^2 & x_3 & 1\\\\\n",
        "x_4^3 & x_4^2 & x_4 & 1\\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "a\\\\ b\\\\ c\\\\ d\\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\y_2 \\\\y_3 \\\\y_4 \\\\\n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.concatenate([xs2**i for i in [3,2,1,0]], axis=1)\n",
        "A"
      ],
      "metadata": {
        "id": "HKRQ3_Tn6bya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = jnp.linalg.inv(A) @ ys2\n",
        "\n",
        "g = lambda x: a*x**3 + b*x**2 + c*x + d\n",
        "\n",
        "print(f'{a.item():.4f}x^3 + {b.item():.4f}x^2 + {c.item():.4f}x + {d.item():.4f}')\n",
        "\n",
        "plt.title('Exact solution')\n",
        "plt.plot(xx, f(xx),label='$y=f(x)$')\n",
        "plt.scatter(xs2, ys2)\n",
        "\n",
        "plt.plot(xx, g(xx), label='$y=g(x)$')\n",
        "plt.scatter(xs2, g(xs2))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efVY00xg7VQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta9ptQOzpHFI"
      },
      "source": [
        "These lessons are just the first stpe of your journey to data science.\n",
        "\n",
        "By just reading the statement of the universal approximation theorem,\n",
        "you can figure out many strenghs and weaknesses of ANN.\n",
        "\n",
        "Good luck.\n"
      ]
    }
  ]
}